{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For parallel processing\n",
    "import parsl\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "# helpers\n",
    "from grouputils import initialize_rasterizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "![](https://raw.githubusercontent.com/PermafrostDiscoveryGateway/viz-raster/develop/docs/images/raster_tldr.png)\n",
    "\n",
    "Today, we will take the regularly gridded staged data from yesterday and create rasters from the vector data (left side of diagram above). Each pixel of the raster will contain a stasticic calculated based on the underlying vector data for that pixel.\n",
    "\n",
    "The two statistics we calculate are:\n",
    "\n",
    "- number of IWP per pixel\n",
    "- proportion of pixel covered by IWP\n",
    "\n",
    "Similar to the `TileStager` from the first step of the group project, in this step we will use a `RasterTiler` class, which we initialize using `initialize_rasterizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwp_rasterizer = initialize_rasterizer(\"/home/shares/example-pdg-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a look at the data we need to rasterize first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "staged_paths = iwp_rasterizer.tiles.get_filenames_from_dir('staged')\n",
    "print(len(staged_paths), \"files to rasterize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `rasterize_vector` method on our `RasterTiler` class object, and pass it the first file in our list of staged files to rasterize just one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwp_rasterizer.rasterize_vector(staged_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This went pretty quickly, but with thousands of files, it would still be beneficial to paralleize. Estimate the length of time it would take to process the files in series below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate duration of rasterization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterize in Parallel\n",
    "\n",
    "Given what you know about the limits of parallelization, discuss in your group whether you think this problem is:\n",
    "\n",
    "- cpu-bound\n",
    "- memory-bound\n",
    "- I/O-bound\n",
    "- network-bound\n",
    "\n",
    "How does it compare to the parallelization task from yesterday?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parallelizing this step over all 2000 files, we will instead process the data in batches of files. Below is a function we can use to make batches of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because rasterization is relatively quick, we want each parsl \"task\" to process a batch of tiles.\n",
    "def make_batch(items, batch_size):\n",
    "    # Create batches of a given size from a list of items.\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `make_batch` function to make batches of 10 items each of our staged data (`staged_paths`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batches of 10 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set up your Parsl executor again, with `max_workers` set at 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Parsl executor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up your Parsl app to run the `rasterize_vector` method in parallel. Remember that Parsl apps cannot rely on global variables or package imports, so you'll need to make sure to pass the app all of the variables it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Parsl app that uses the rasterize_vectors method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the app in parallel over all of the batches of files you created previously. Don't forget to add a line to shut down the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the batches in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set your executor up correctly, this should have finished in around 18 minutes. Check to make sure that you have the same number of GeoTIFF files as we do staged vector tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_paths = iwp_rasterizer.tiles.get_filenames_from_dir('geotiff')\n",
    "len(geotiff_paths) == len(staged_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Thinking back to the group project yesterday, how would you test whether this problem is CPU or I/O bound? One option would be to test whether adding more CPU decreases your computation time. Set up a test on a subset of the staged files (the first 500 is fine) and time the process at different `max_workers`. To make sure you don't overload the server, do not exceed 32 workers in your tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scomp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bc86895b5d8569f33588bfbb1475393b4e3b52b49eb441663b259ecef6692f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
